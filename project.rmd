# Practical Machine Learning - Course Project

##Aim of the Project
The aim of the project is to build a machine learning model from the sample data that is attempting to most accurately predict the manner in which the exercise was performed by the data collected via quantified Self devices. This is a classification problem into discrete categories, which in the training data are located in the 'classe' variable.

##initial settings 
```{r}
echo = TRUE  # visible code chunks
options(scipen = 1)  # Turn off scientific notations for numbers
set.seed(76543)
```

##Include Libraries
```{r}
library(rattle)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
```


##Download the training data
```{r}
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "./data/data-train.csv"
testFile  <- "./data/data-test.csv"
if (!file.exists("./data")) {
  dir.create("./data")
}
if (!file.exists(trainFile)) {
  download.file(trainUrl, destfile = trainFile, method = "curl")
}
if (!file.exists(testFile)) {
  download.file(testUrl, destfile = testFile, method = "curl")
}
rm(trainUrl)
rm(testUrl)
```


##Load data
```{r}
trainRaw <- read.csv(trainFile)
testRaw <- read.csv(testFile)
dim(trainRaw) 
dim(testRaw)
rm(trainFile)
rm(testFile)
```

##Clean data
1. Remove the columns where variance is near zero.
```{r}
NZV <- nearZeroVar(trainRaw, saveMetrics = TRUE)
head(NZV, 20)
training01 <- trainRaw[, !NZV$nzv]
testing01 <- testRaw[, !NZV$nzv]
dim(training01)
dim(testing01)
rm(trainRaw)
rm(testRaw)
rm(NZV)
```

2. Remove also specific dimensional variables that may not be pertinent to the prediction model.
```{r}
regex <- grepl("^X|timestamp|user_name", names(training01))
training <- training01[, !regex]
testing <- testing01[, !regex]
rm(regex)
rm(training01)
rm(testing01)

```


3. Remove final NA's
```{r}
cond <- (colSums(is.na(training)) == 0)
training <- training[, cond]
testing <- testing[, cond]
rm(cond)
dim(training)
dim(testing)
```

Now, the cleaned training data set contains 19622 observations and 54 variables, while the testing data set contains 20 observations and 54 variables.

##Splitting Dataset
```{r}
index <- createDataPartition(training$classe, p = 0.70, list = FALSE)
validation <- training[-index, ]
training <- training[index, ]
rm(index)
```


##Modelling
1. Decision Tree

We fit a predictive model for activity recognition using Decision Tree algorithm.
```{r}
modelTree <- rpart(classe ~ ., data = training, method = "class")
prp(modelTree)
```

Now, we estimate the performance of the model on the validation data set.
```{r}
predictTree <- predict(modelTree, validation, type = "class")
confusionMatrix(validation$classe, predictTree)
accuracy <- postResample(predictTree, validation$classe)
ose <- 1 - as.numeric(confusionMatrix(validation$classe, predictTree)$overall[1])
rm(predictTree)
rm(modelTree)
```
The Estimated Accuracy of the Random Forest Model is 73.93% and the Estimated Out-of-Sample Error is 26.07%.

2. Boosting Algorithm
```{r}
boostFit <- train(classe ~ ., method = "gbm", data = training, verbose = F, trControl = trainControl(method = "cv", number = 10))
boostFit
plot(boostFit, ylim = c(0.9, 1))
```
The Estimated Accuracy of the  Model is 98.79165%


3. Random Forest

Random Forest algorithm, 5 Cross Validation will be used.
```{r}
modelRF <- train(classe ~ ., data = training, method = "rf", trControl = trainControl(method = "cv", 5), ntree = 250)
modelRF
```

Now, we estimate the performance of the model on the validation data set.
```{r}
predictRF <- predict(modelRF, validation)
confusionMatrix(validation$classe, predictRF)
accuracy <- postResample(predictRF, validation$classe)
ose <- 1 - as.numeric(confusionMatrix(validation$classe, predictRF)$overall[1])
rm(predictRF)
```

The Estimated Accuracy of the Random Forest Model is 99.73% and the Estimated Out-of-Sample Error is 0.27%.
Random Forests yielded better Results even from boosting model.

```{r}
rm(accuracy)
rm(ose)
predict(modelRF, testing[, -length(names(testing))])
```

#Conclusion
The decition tree has an Accuracy of 74%. Boosting model and random forrest are on same scale and the latter is quite better with accuracy of 99%. Random forest  will be used for predicting the test set.

##Project submission files
```{r}

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("./Assignment_Solutions/problem_id_",i,".txt")
    write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, col.names = FALSE)
  }
}
```

##Generate Data for Submitting
```{r}
pml_write_files(predict(modelRF, testing[, -length(names(testing))]))
rm(modelRF)
rm(training)
rm(testing)
rm(validation)
rm(pml_write_files)
```